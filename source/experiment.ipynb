{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom imports\n",
    "from config import get_config\n",
    "from dataset import DataSet, prepare_interaction_pairs, create_tf_dataset\n",
    "from lstm_model import build_lstm_model\n",
    "from lstm_attention_model import build_lstm_attention_model\n",
    "from transformer_model import build_transformer_model\n",
    "\n",
    "# Cell 1: Define CI score function\n",
    "def ci_score(y_true, y_pred):\n",
    "    ind = np.argsort(y_true)\n",
    "    y_true = y_true[ind]\n",
    "    y_pred = y_pred[ind]\n",
    "    i = len(y_true)-1\n",
    "    j = i-1\n",
    "    z = 0\n",
    "    S = 0\n",
    "    while i > 0:\n",
    "        while j >= 0:\n",
    "            if y_true[i] > y_true[j]:\n",
    "                z = z+1\n",
    "                if y_pred[i] > y_pred[j]:\n",
    "                    S = S + 1\n",
    "            j = j - 1\n",
    "        i = i - 1\n",
    "        j = i-1\n",
    "    ci = S/z\n",
    "    return ci\n",
    "\n",
    "# Cell 2: Load configuration and dataset\n",
    "config = get_config()\n",
    "dataset_type = 'davis'  # Change to 'kiba' for KIBA dataset\n",
    "\n",
    "if dataset_type == 'davis':\n",
    "    dataset = DataSet(config['davis_path'], config['problem_type'], config['max_seq_len'],\n",
    "                      config['max_smi_len'], 'davis', config['davis_convert_to_log'])\n",
    "else:\n",
    "    dataset = DataSet(config['kiba_path'], config['problem_type'], config['max_seq_len'],\n",
    "                      config['max_smi_len'], 'kiba', config['kiba_convert_to_log'])\n",
    "\n",
    "XD, XT, Y, label_row_inds, label_col_inds, _, _ = dataset.get_data()\n",
    "\n",
    "# Cell 3: Define models to be tested\n",
    "models = {\n",
    "    'LSTM': build_lstm_model,\n",
    "    'LSTM with Attention': build_lstm_attention_model,\n",
    "    'Transformer': build_transformer_model\n",
    "}\n",
    "\n",
    "# Cell 4: Experiment setup\n",
    "results = {}\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cell 5: Run experiments\n",
    "for model_name, model_builder in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    model_ci_scores = []\n",
    "    model_rmse_scores = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(label_row_inds)):\n",
    "        print(f\"Fold {fold + 1}/5\")\n",
    "\n",
    "        train_drug_data, train_target_data, train_affinity = prepare_interaction_pairs(\n",
    "            XD, XT, Y, label_row_inds[train_index], label_col_inds[train_index])\n",
    "\n",
    "        val_drug_data, val_target_data, val_affinity = prepare_interaction_pairs(\n",
    "            XD, XT, Y, label_row_inds[val_index], label_col_inds[val_index])\n",
    "\n",
    "        train_dataset = create_tf_dataset(train_drug_data, train_target_data, train_affinity, config['batch_size'])\n",
    "        val_dataset = create_tf_dataset(val_drug_data, val_target_data, val_affinity, config['batch_size'], shuffle=False)\n",
    "\n",
    "        model = model_builder(config)\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=config['num_epoch'],\n",
    "            validation_data=val_dataset,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        val_predictions = model.predict(val_dataset)\n",
    "        val_ci = ci_score(val_affinity, val_predictions.flatten())\n",
    "        val_rmse = tf.keras.metrics.RootMeanSquaredError()(val_affinity, val_predictions.flatten()).numpy()\n",
    "\n",
    "        model_ci_scores.append(val_ci)\n",
    "        model_rmse_scores.append(val_rmse)\n",
    "\n",
    "        print(f\"Fold {fold + 1} - CI: {val_ci:.4f}, RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "    avg_ci = np.mean(model_ci_scores)\n",
    "    avg_rmse = np.mean(model_rmse_scores)\n",
    "    results[model_name] = {'CI': avg_ci, 'RMSE': avg_rmse}\n",
    "\n",
    "    print(f\"{model_name} - Average CI: {avg_ci:.4f}, Average RMSE: {avg_rmse:.4f}\")\n",
    "\n",
    "# Cell 6: Compare results\n",
    "print(\"\\nFinal Results:\")\n",
    "for model_name, scores in results.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  CI: {scores['CI']:.4f}\")\n",
    "    print(f\"  RMSE: {scores['RMSE']:.4f}\")\n",
    "\n",
    "# Cell 7: Visualize results\n",
    "def plot_results(results):\n",
    "    models = list(results.keys())\n",
    "    ci_scores = [results[model]['CI'] for model in models]\n",
    "    rmse_scores = [results[model]['RMSE'] for model in models]\n",
    "\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    rects1 = ax1.bar(x - width/2, ci_scores, width, label='CI', color='b', alpha=0.7)\n",
    "    rects2 = ax2.bar(x + width/2, rmse_scores, width, label='RMSE', color='r', alpha=0.7)\n",
    "\n",
    "    ax1.set_xlabel('Models')\n",
    "    ax1.set_ylabel('CI Score')\n",
    "    ax2.set_ylabel('RMSE')\n",
    "    ax1.set_title('Model Performance Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models, rotation=45, ha='right')\n",
    "\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_results(results)\n",
    "\n",
    "# Cell 8: Compare with DeepDTA results\n",
    "deepdta_results = {\n",
    "    'Davis': {'CI': 0.878, 'MSE': 0.261},\n",
    "    'KIBA': {'CI': 0.863, 'MSE': 0.194}\n",
    "}\n",
    "\n",
    "print(f\"\\nDeepDTA results on {dataset_type.capitalize()} dataset:\")\n",
    "print(f\"CI: {deepdta_results[dataset_type.capitalize()]['CI']:.4f}\")\n",
    "print(f\"MSE: {deepdta_results[dataset_type.capitalize()]['MSE']:.4f}\")\n",
    "\n",
    "print(\"\\nComparison with DeepDTA:\")\n",
    "for model_name, scores in results.items():\n",
    "    ci_diff = scores['CI'] - deepdta_results[dataset_type.capitalize()]['CI']\n",
    "    rmse_diff = scores['RMSE'] - np.sqrt(deepdta_results[dataset_type.capitalize()]['MSE'])\n",
    "\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  CI difference: {ci_diff:.4f} ({'better' if ci_diff > 0 else 'worse'})\")\n",
    "    print(f\"  RMSE difference: {rmse_diff:.4f} ({'worse' if rmse_diff > 0 else 'better'})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
