{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y1GTTReXCXAS",
    "outputId": "48a67509-8647-4077-ceff-b39392178d8f"
   },
   "id": "y1GTTReXCXAS",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/BioInf-Final/')"
   ],
   "metadata": {
    "id": "0LVXjgAhCp9i"
   },
   "id": "0LVXjgAhCp9i",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-18T15:07:39.923501Z",
     "start_time": "2024-08-18T15:07:29.738858Z"
    },
    "id": "initial_id"
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "from source.dataset import DataSet, prepare_interaction_pairs\n",
    "\n",
    "def concordance_index(y_true, y_pred):\n",
    "    pairs = [(i, j) for i in range(len(y_true)) for j in range(i+1, len(y_true)) if y_true[i] != y_true[j]]\n",
    "    concordant = sum((y_true[i] > y_true[j]) == (y_pred[i] > y_pred[j]) for i, j in pairs)\n",
    "    return concordant / len(pairs)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            drugs, proteins, labels = batch['drug'].to(device), batch['target'].to(device), batch['affinity'].to(device)\n",
    "            # drugs, proteins, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(drugs, proteins)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    mse = total_loss / len(loader)\n",
    "    ci = concordance_index(all_labels, all_preds)\n",
    "    return mse, ci\n",
    "\n",
    "def unite_lists(list_of_lists):\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_reproducible_dataloader(dataset, batch_size, shuffle=True, seed=42):\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(seed)\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,  # Set to 0 for full reproducibility\n",
    "        generator=generator,\n",
    "        worker_init_fn=seed_worker\n",
    "    )\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def run_experiment(model_class, config, train_dataset, test_dataset, dataset, num_epochs=100):\n",
    "    set_seeds(seed=42)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Set up data loaders\n",
    "    train_loader = create_reproducible_dataloader(train_dataset, batch_size=config['batch_size'])\n",
    "    test_loader = create_reproducible_dataloader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "    # Initialize model\n",
    "    model = model_class(config).to(device)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    # Training loop\n",
    "    best_test_mse = float('inf')\n",
    "    best_test_ci = 0\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    # for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            drugs, proteins, labels = batch['drug'].to(device), batch['target'].to(device), batch['affinity'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(drugs, proteins)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                drugs, proteins, labels = batch['drug'].to(device), batch['target'].to(device), batch['affinity'].to(device)\n",
    "                outputs = model(drugs, proteins)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        test_loss /= len(test_loader)\n",
    "        test_ci = concordance_index(all_labels, all_preds)\n",
    "\n",
    "        if test_loss < best_test_mse:\n",
    "            best_test_mse = test_loss\n",
    "            best_test_ci = test_ci\n",
    "            torch.save(model.state_dict(), f'{model_class.__name__}_{dataset}_best_model.pt')\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Test MSE: {test_loss:.4f}, Test CI: {test_ci:.4f}\")\n",
    "\n",
    "    print(f\"Best Test MSE: {best_test_mse:.4f}, Best Test CI: {best_test_ci:.4f}\")\n",
    "    return best_test_mse, best_test_ci"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T15:07:45.591909Z",
     "start_time": "2024-08-18T15:07:45.577466Z"
    },
    "id": "18b6954be81e78c7"
   },
   "cell_type": "code",
   "source": [
    "class DTIDataset(Dataset):\n",
    "    def __init__(self, drugs, targets, affinity):\n",
    "        self.drugs = torch.LongTensor(drugs)\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "        self.affinity = torch.FloatTensor(affinity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.affinity)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'drug': self.drugs[idx],\n",
    "            'target': self.targets[idx],\n",
    "            'affinity': self.affinity[idx]\n",
    "        }\n",
    "\n",
    "def get_datasets(dataset_type, config):\n",
    "  if dataset_type == 'davis':\n",
    "      dataset = DataSet(config['davis_path'], config['problem_type'], config['max_seq_len'],\n",
    "                        config['max_smi_len'], 'davis', config['davis_convert_to_log'])\n",
    "  else:\n",
    "      dataset = DataSet(config['kiba_path'], config['problem_type'], config['max_seq_len'],\n",
    "                        config['max_smi_len'], 'kiba', config['kiba_convert_to_log'])\n",
    "\n",
    "  XD, XT, Y, label_row_inds, label_col_inds, test_fold, train_folds = dataset.get_data()\n",
    "\n",
    "  train_fold = unite_lists(train_folds)\n",
    "\n",
    "  train_drugs, train_targets, train_affinity = prepare_interaction_pairs(XD, XT, Y, label_row_inds[train_fold], label_col_inds[train_fold])\n",
    "  test_drugs, test_targets, test_affinity = prepare_interaction_pairs(XD, XT, Y, label_row_inds[test_fold], label_col_inds[test_fold])\n",
    "\n",
    "  train_dataset = DTIDataset(train_drugs, train_targets, train_affinity)\n",
    "  test_dataset = DTIDataset(test_drugs, test_targets, test_affinity)\n",
    "  return train_dataset, test_dataset"
   ],
   "id": "18b6954be81e78c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.drug_embedding = nn.Embedding(config['charsmiset_size'] + 1, config['embed_dim'])\n",
    "        self.protein_embedding = nn.Embedding(config['charseqset_size'] + 1, config['embed_dim'])\n",
    "\n",
    "        # Single layer LSTM with reduced hidden size\n",
    "        self.drug_lstm = nn.LSTM(config['embed_dim'], config['lstm_dim'], num_layers=config['lstm_layers'], batch_first=True)\n",
    "        self.protein_lstm = nn.LSTM(config['embed_dim'], config['lstm_dim'], num_layers=config['lstm_layers'], batch_first=True)\n",
    "\n",
    "        # Reduced number of fully connected layers\n",
    "        self.fc1 = nn.Linear(config['lstm_dim'] * 2, 32)  # 64 because we concatenate two 32-dim vectors\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(config['dropout_rate'])\n",
    "\n",
    "    def forward(self, drug, protein):\n",
    "        drug_embedded = self.drug_embedding(drug)\n",
    "        protein_embedded = self.protein_embedding(protein)\n",
    "\n",
    "        _, (drug_hidden, _) = self.drug_lstm(drug_embedded)\n",
    "        _, (protein_hidden, _) = self.protein_lstm(protein_embedded)\n",
    "\n",
    "        # Use only the last hidden state\n",
    "        drug_hidden = drug_hidden[-1]\n",
    "        protein_hidden = protein_hidden[-1]\n",
    "\n",
    "        concat = torch.cat([drug_hidden, protein_hidden], dim=1)\n",
    "\n",
    "        fc1 = self.relu(self.fc1(concat))\n",
    "        fc1 = self.dropout(fc1)\n",
    "\n",
    "        output = self.fc2(fc1).squeeze(-1)\n",
    "        return output\n",
    "\n",
    "config = {\n",
    "  'charsmiset_size': 64,\n",
    "  'charseqset_size': 26,\n",
    "  'embed_dim': 128,\n",
    "  'lstm_dim': 64,\n",
    "  'lstm_layers': 2,\n",
    "  'dropout_rate': 0.3,\n",
    "  'learning_rate': 0.001,\n",
    "  'batch_size': 128,\n",
    "\n",
    "  # Dataset specific\n",
    "  'davis_convert_to_log': True,\n",
    "  'kiba_convert_to_log': False,\n",
    "\n",
    "  # Input dimensions\n",
    "  'max_seq_len': 1000,\n",
    "  'max_smi_len': 100,\n",
    "  'davis_path': '/content/drive/MyDrive/BioInf-Final/data/davis/',\n",
    "  'kiba_path': '/content/drive/MyDrive/BioInf-Final/data/kiba/',\n",
    "  'problem_type': 1\n",
    "\n",
    "}\n",
    "\n",
    "train_dataset, test_dataset = get_datasets('davis', config)\n",
    "\n",
    "# Run experiment with SimpleLSTM\n",
    "mse, ci = run_experiment(SimpleLSTM, config, train_dataset, test_dataset, num_epochs=100)\n",
    "print(f\"SimpleLSTM - Final MSE: {mse:.4f}, Final CI: {ci:.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "maw4PYCXiX2F",
    "outputId": "c39c668e-e9a5-4b93-d9da-5c6fdf6342be"
   },
   "id": "maw4PYCXiX2F",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100, Train Loss: 3.2424, Test MSE: 0.7971, Test CI: 0.5373\n",
      "Epoch 2/100, Train Loss: 1.4436, Test MSE: 0.7826, Test CI: 0.5644\n",
      "Epoch 3/100, Train Loss: 1.4412, Test MSE: 0.7781, Test CI: 0.5782\n",
      "Epoch 4/100, Train Loss: 1.4116, Test MSE: 0.8006, Test CI: 0.5729\n",
      "Epoch 5/100, Train Loss: 1.4109, Test MSE: 0.7697, Test CI: 0.5857\n",
      "Epoch 6/100, Train Loss: 1.4039, Test MSE: 0.7608, Test CI: 0.5931\n",
      "Epoch 7/100, Train Loss: 1.3850, Test MSE: 0.7591, Test CI: 0.5878\n",
      "Epoch 8/100, Train Loss: 1.3563, Test MSE: 0.7659, Test CI: 0.5821\n",
      "Epoch 9/100, Train Loss: 1.3480, Test MSE: 0.7712, Test CI: 0.5911\n",
      "Epoch 10/100, Train Loss: 1.3279, Test MSE: 0.7644, Test CI: 0.5901\n",
      "Epoch 11/100, Train Loss: 1.3016, Test MSE: 0.7826, Test CI: 0.5932\n",
      "Epoch 12/100, Train Loss: 1.2705, Test MSE: 0.7602, Test CI: 0.5986\n",
      "Epoch 13/100, Train Loss: 1.2447, Test MSE: 0.8390, Test CI: 0.6018\n",
      "Epoch 14/100, Train Loss: 1.2292, Test MSE: 0.7494, Test CI: 0.6000\n",
      "Epoch 15/100, Train Loss: 1.2167, Test MSE: 0.7544, Test CI: 0.6000\n",
      "Epoch 16/100, Train Loss: 1.1988, Test MSE: 0.8118, Test CI: 0.5952\n",
      "Epoch 17/100, Train Loss: 1.1786, Test MSE: 0.7831, Test CI: 0.5964\n",
      "Epoch 18/100, Train Loss: 1.1572, Test MSE: 0.7461, Test CI: 0.6022\n",
      "Epoch 19/100, Train Loss: 1.1420, Test MSE: 0.7497, Test CI: 0.6053\n",
      "Epoch 20/100, Train Loss: 1.1126, Test MSE: 0.7488, Test CI: 0.6039\n",
      "Epoch 21/100, Train Loss: 1.0984, Test MSE: 0.7544, Test CI: 0.6023\n",
      "Epoch 22/100, Train Loss: 1.0677, Test MSE: 0.7597, Test CI: 0.6073\n",
      "Epoch 23/100, Train Loss: 1.0467, Test MSE: 0.7687, Test CI: 0.5968\n",
      "Epoch 24/100, Train Loss: 1.0361, Test MSE: 0.7588, Test CI: 0.6082\n",
      "Epoch 25/100, Train Loss: 1.0136, Test MSE: 0.7714, Test CI: 0.6070\n",
      "Epoch 26/100, Train Loss: 1.0035, Test MSE: 0.7513, Test CI: 0.6147\n",
      "Epoch 27/100, Train Loss: 0.9962, Test MSE: 0.7531, Test CI: 0.6251\n",
      "Epoch 28/100, Train Loss: 0.9575, Test MSE: 0.7462, Test CI: 0.6362\n",
      "Epoch 29/100, Train Loss: 0.9500, Test MSE: 0.7496, Test CI: 0.6395\n",
      "Epoch 30/100, Train Loss: 0.9376, Test MSE: 0.7295, Test CI: 0.6416\n",
      "Epoch 31/100, Train Loss: 0.9149, Test MSE: 0.7289, Test CI: 0.6465\n",
      "Epoch 32/100, Train Loss: 0.9003, Test MSE: 0.7335, Test CI: 0.6457\n",
      "Epoch 33/100, Train Loss: 0.8838, Test MSE: 0.7442, Test CI: 0.6486\n",
      "Epoch 34/100, Train Loss: 0.8674, Test MSE: 0.7312, Test CI: 0.6479\n",
      "Epoch 35/100, Train Loss: 0.8557, Test MSE: 0.7304, Test CI: 0.6495\n",
      "Epoch 36/100, Train Loss: 0.8555, Test MSE: 0.7268, Test CI: 0.6490\n",
      "Epoch 37/100, Train Loss: 0.8445, Test MSE: 0.7282, Test CI: 0.6537\n",
      "Epoch 38/100, Train Loss: 0.8377, Test MSE: 0.7487, Test CI: 0.6540\n",
      "Epoch 39/100, Train Loss: 0.8220, Test MSE: 0.7294, Test CI: 0.6497\n",
      "Epoch 40/100, Train Loss: 0.8193, Test MSE: 0.7266, Test CI: 0.6526\n",
      "Epoch 41/100, Train Loss: 0.8132, Test MSE: 0.7281, Test CI: 0.6533\n",
      "Epoch 42/100, Train Loss: 0.8019, Test MSE: 0.7300, Test CI: 0.6542\n",
      "Epoch 43/100, Train Loss: 0.7902, Test MSE: 0.7318, Test CI: 0.6538\n",
      "Epoch 44/100, Train Loss: 0.7888, Test MSE: 0.7269, Test CI: 0.6565\n",
      "Epoch 45/100, Train Loss: 0.7812, Test MSE: 0.7276, Test CI: 0.6547\n",
      "Epoch 46/100, Train Loss: 0.7716, Test MSE: 0.7294, Test CI: 0.6510\n",
      "Epoch 47/100, Train Loss: 0.7647, Test MSE: 0.7303, Test CI: 0.6549\n",
      "Epoch 48/100, Train Loss: 0.7622, Test MSE: 0.7353, Test CI: 0.6544\n",
      "Epoch 49/100, Train Loss: 0.7658, Test MSE: 0.7261, Test CI: 0.6567\n",
      "Epoch 50/100, Train Loss: 0.7619, Test MSE: 0.7291, Test CI: 0.6567\n",
      "Epoch 51/100, Train Loss: 0.7497, Test MSE: 0.7270, Test CI: 0.6548\n",
      "Epoch 52/100, Train Loss: 0.7500, Test MSE: 0.7246, Test CI: 0.6573\n",
      "Epoch 53/100, Train Loss: 0.7476, Test MSE: 0.7275, Test CI: 0.6529\n",
      "Epoch 54/100, Train Loss: 0.7430, Test MSE: 0.7256, Test CI: 0.6571\n",
      "Epoch 55/100, Train Loss: 0.7412, Test MSE: 0.7339, Test CI: 0.6550\n",
      "Epoch 56/100, Train Loss: 0.7381, Test MSE: 0.7242, Test CI: 0.6572\n",
      "Epoch 57/100, Train Loss: 0.7337, Test MSE: 0.7256, Test CI: 0.6568\n",
      "Epoch 58/100, Train Loss: 0.7346, Test MSE: 0.7278, Test CI: 0.6572\n",
      "Epoch 59/100, Train Loss: 0.7340, Test MSE: 0.7271, Test CI: 0.6563\n",
      "Epoch 60/100, Train Loss: 0.7268, Test MSE: 0.7234, Test CI: 0.6583\n",
      "Epoch 61/100, Train Loss: 0.7276, Test MSE: 0.7250, Test CI: 0.6578\n",
      "Epoch 62/100, Train Loss: 0.7273, Test MSE: 0.7248, Test CI: 0.6568\n",
      "Epoch 63/100, Train Loss: 0.7236, Test MSE: 0.7254, Test CI: 0.6569\n",
      "Epoch 64/100, Train Loss: 0.7221, Test MSE: 0.7268, Test CI: 0.6585\n",
      "Epoch 65/100, Train Loss: 0.7204, Test MSE: 0.7261, Test CI: 0.6558\n",
      "Epoch 66/100, Train Loss: 0.7194, Test MSE: 0.7270, Test CI: 0.6548\n",
      "Epoch 67/100, Train Loss: 0.7178, Test MSE: 0.7290, Test CI: 0.6559\n",
      "Epoch 68/100, Train Loss: 0.7203, Test MSE: 0.7265, Test CI: 0.6560\n",
      "Epoch 69/100, Train Loss: 0.7169, Test MSE: 0.7245, Test CI: 0.6562\n",
      "Epoch 70/100, Train Loss: 0.7153, Test MSE: 0.7277, Test CI: 0.6569\n",
      "Epoch 71/100, Train Loss: 0.7148, Test MSE: 0.7300, Test CI: 0.6546\n",
      "Epoch 72/100, Train Loss: 0.7134, Test MSE: 0.7245, Test CI: 0.6565\n",
      "Epoch 73/100, Train Loss: 0.7144, Test MSE: 0.7298, Test CI: 0.6579\n",
      "Epoch 74/100, Train Loss: 0.7149, Test MSE: 0.7234, Test CI: 0.6570\n",
      "Epoch 75/100, Train Loss: 0.7158, Test MSE: 0.7239, Test CI: 0.6587\n",
      "Epoch 76/100, Train Loss: 0.7124, Test MSE: 0.7244, Test CI: 0.6572\n",
      "Epoch 77/100, Train Loss: 0.7119, Test MSE: 0.7245, Test CI: 0.6577\n",
      "Epoch 78/100, Train Loss: 0.7131, Test MSE: 0.7262, Test CI: 0.6565\n",
      "Epoch 79/100, Train Loss: 0.7113, Test MSE: 0.7241, Test CI: 0.6583\n",
      "Epoch 80/100, Train Loss: 0.7115, Test MSE: 0.7234, Test CI: 0.6563\n",
      "Epoch 81/100, Train Loss: 0.7114, Test MSE: 0.7222, Test CI: 0.6588\n",
      "Epoch 82/100, Train Loss: 0.7079, Test MSE: 0.7235, Test CI: 0.6577\n",
      "Epoch 83/100, Train Loss: 0.7143, Test MSE: 0.7248, Test CI: 0.6560\n",
      "Epoch 84/100, Train Loss: 0.7098, Test MSE: 0.7257, Test CI: 0.6546\n",
      "Epoch 85/100, Train Loss: 0.6605, Test MSE: 0.6215, Test CI: 0.7200\n",
      "Epoch 86/100, Train Loss: 0.5776, Test MSE: 0.5596, Test CI: 0.7504\n",
      "Epoch 87/100, Train Loss: 0.5387, Test MSE: 0.5398, Test CI: 0.7609\n",
      "Epoch 88/100, Train Loss: 0.5124, Test MSE: 0.5240, Test CI: 0.7741\n",
      "Epoch 89/100, Train Loss: 0.4939, Test MSE: 0.5167, Test CI: 0.7827\n",
      "Epoch 90/100, Train Loss: 0.4837, Test MSE: 0.5091, Test CI: 0.7885\n",
      "Epoch 91/100, Train Loss: 0.4775, Test MSE: 0.4978, Test CI: 0.7970\n",
      "Epoch 92/100, Train Loss: 0.4693, Test MSE: 0.4990, Test CI: 0.7962\n",
      "Epoch 93/100, Train Loss: 0.4673, Test MSE: 0.4961, Test CI: 0.7984\n",
      "Epoch 94/100, Train Loss: 0.4664, Test MSE: 0.4922, Test CI: 0.8053\n",
      "Epoch 95/100, Train Loss: 0.4587, Test MSE: 0.4919, Test CI: 0.8036\n",
      "Epoch 96/100, Train Loss: 0.4591, Test MSE: 0.4895, Test CI: 0.8075\n",
      "Epoch 97/100, Train Loss: 0.4551, Test MSE: 0.4854, Test CI: 0.8112\n",
      "Epoch 98/100, Train Loss: 0.4541, Test MSE: 0.4900, Test CI: 0.8095\n",
      "Epoch 99/100, Train Loss: 0.4521, Test MSE: 0.4849, Test CI: 0.8152\n",
      "Epoch 100/100, Train Loss: 0.4539, Test MSE: 0.4833, Test CI: 0.8174\n",
      "Best Test MSE: 0.4833, Best Test CI: 0.8174\n",
      "SimpleLSTM - Final MSE: 0.4833, Final CI: 0.8174\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.drug_embedding = nn.Embedding(config['charsmiset_size'] + 1, config['embed_dim'])\n",
    "        self.protein_embedding = nn.Embedding(config['charseqset_size'] + 1, config['embed_dim'])\n",
    "\n",
    "        self.drug_lstm = nn.LSTM(config['embed_dim'], config['lstm_dim'],\n",
    "                                 num_layers=config['lstm_layers'], batch_first=True, bidirectional=True)\n",
    "        self.protein_lstm = nn.LSTM(config['embed_dim'], config['lstm_dim'],\n",
    "                                    num_layers=config['lstm_layers'], batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(config['lstm_dim'] * 4, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(config['dropout_rate'])\n",
    "\n",
    "    def forward(self, drug, protein):\n",
    "        drug_embedded = self.drug_embedding(drug)\n",
    "        protein_embedded = self.protein_embedding(protein)\n",
    "\n",
    "        _, (drug_hidden, _) = self.drug_lstm(drug_embedded)\n",
    "        _, (protein_hidden, _) = self.protein_lstm(protein_embedded)\n",
    "\n",
    "        drug_hidden = torch.cat((drug_hidden[-2], drug_hidden[-1]), dim=1)\n",
    "        protein_hidden = torch.cat((protein_hidden[-2], protein_hidden[-1]), dim=1)\n",
    "\n",
    "        concat = torch.cat([drug_hidden, protein_hidden], dim=1)\n",
    "\n",
    "        fc1 = self.relu(self.fc1(concat))\n",
    "        fc1 = self.dropout(fc1)\n",
    "\n",
    "        output = self.fc2(fc1).squeeze(-1)\n",
    "        return output\n",
    "\n",
    "config = {\n",
    "  'charsmiset_size': 64,\n",
    "  'charseqset_size': 26,\n",
    "  'embed_dim': 128,\n",
    "  'lstm_dim': 64,\n",
    "  'lstm_layers': 2,\n",
    "  'dropout_rate': 0.3,\n",
    "  'learning_rate': 0.0005,\n",
    "  'batch_size': 128,\n",
    "\n",
    "  # Dataset specific\n",
    "  'davis_convert_to_log': True,\n",
    "  'kiba_convert_to_log': False,\n",
    "\n",
    "  # Input dimensions\n",
    "  'max_seq_len': 1000,\n",
    "  'max_smi_len': 100,\n",
    "  'davis_path': '/content/drive/MyDrive/BioInf-Final/data/davis/',\n",
    "  'kiba_path': '/content/drive/MyDrive/BioInf-Final/data/kiba/',\n",
    "  'problem_type': 1\n",
    "\n",
    "}\n",
    "\n",
    "train_dataset, test_dataset = get_datasets('davis', config)\n",
    "\n",
    "# Run experiment with BidirectionalLSTM\n",
    "mse, ci = run_experiment(BidirectionalLSTM, config, train_dataset, test_dataset, num_epochs=100)\n",
    "print(f\"BidirectionalLSTM - Final MSE: {mse:.4f}, Final CI: {ci:.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1TmJcN9C3RHK",
    "outputId": "43ee7478-5da9-424b-c1ab-35d3482d5463"
   },
   "id": "1TmJcN9C3RHK",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100, Train Loss: 3.8459, Test MSE: 0.7826, Test CI: 0.5918\n",
      "Epoch 2/100, Train Loss: 1.1685, Test MSE: 0.6755, Test CI: 0.7141\n",
      "Epoch 3/100, Train Loss: 1.0384, Test MSE: 0.6323, Test CI: 0.7431\n",
      "Epoch 4/100, Train Loss: 1.0268, Test MSE: 0.6983, Test CI: 0.7613\n",
      "Epoch 5/100, Train Loss: 1.0011, Test MSE: 0.6050, Test CI: 0.7670\n",
      "Epoch 6/100, Train Loss: 0.9823, Test MSE: 0.6106, Test CI: 0.7732\n",
      "Epoch 7/100, Train Loss: 0.9913, Test MSE: 0.5937, Test CI: 0.7777\n",
      "Epoch 8/100, Train Loss: 0.9818, Test MSE: 0.5914, Test CI: 0.7802\n",
      "Epoch 9/100, Train Loss: 0.9552, Test MSE: 0.5918, Test CI: 0.7828\n",
      "Epoch 10/100, Train Loss: 0.9618, Test MSE: 0.5818, Test CI: 0.7821\n",
      "Epoch 11/100, Train Loss: 0.9467, Test MSE: 0.5954, Test CI: 0.7806\n",
      "Epoch 12/100, Train Loss: 0.9537, Test MSE: 0.6222, Test CI: 0.7836\n",
      "Epoch 13/100, Train Loss: 0.9395, Test MSE: 0.6300, Test CI: 0.7818\n",
      "Epoch 14/100, Train Loss: 0.9346, Test MSE: 0.5784, Test CI: 0.7873\n",
      "Epoch 15/100, Train Loss: 0.9202, Test MSE: 0.5649, Test CI: 0.7889\n",
      "Epoch 16/100, Train Loss: 0.9052, Test MSE: 0.6300, Test CI: 0.7894\n",
      "Epoch 17/100, Train Loss: 0.9236, Test MSE: 0.5804, Test CI: 0.7871\n",
      "Epoch 18/100, Train Loss: 0.8923, Test MSE: 0.5602, Test CI: 0.7901\n",
      "Epoch 19/100, Train Loss: 0.9016, Test MSE: 0.5892, Test CI: 0.7909\n",
      "Epoch 20/100, Train Loss: 0.8869, Test MSE: 0.5813, Test CI: 0.7891\n",
      "Epoch 21/100, Train Loss: 0.8970, Test MSE: 0.5667, Test CI: 0.7917\n",
      "Epoch 22/100, Train Loss: 0.8885, Test MSE: 0.5645, Test CI: 0.7910\n",
      "Epoch 23/100, Train Loss: 0.8706, Test MSE: 0.5731, Test CI: 0.7890\n",
      "Epoch 24/100, Train Loss: 0.8635, Test MSE: 0.5573, Test CI: 0.7922\n",
      "Epoch 25/100, Train Loss: 0.8610, Test MSE: 0.5734, Test CI: 0.7913\n",
      "Epoch 26/100, Train Loss: 0.8492, Test MSE: 0.5578, Test CI: 0.7936\n",
      "Epoch 27/100, Train Loss: 0.8431, Test MSE: 0.5589, Test CI: 0.7918\n",
      "Epoch 28/100, Train Loss: 0.8482, Test MSE: 0.5659, Test CI: 0.7917\n",
      "Epoch 29/100, Train Loss: 0.8330, Test MSE: 0.5762, Test CI: 0.7924\n",
      "Epoch 30/100, Train Loss: 0.8257, Test MSE: 0.5608, Test CI: 0.7911\n",
      "Epoch 31/100, Train Loss: 0.8345, Test MSE: 0.5557, Test CI: 0.7908\n",
      "Epoch 32/100, Train Loss: 0.8140, Test MSE: 0.5551, Test CI: 0.7928\n",
      "Epoch 33/100, Train Loss: 0.8143, Test MSE: 0.5593, Test CI: 0.7912\n",
      "Epoch 34/100, Train Loss: 0.8041, Test MSE: 0.5644, Test CI: 0.7923\n",
      "Epoch 35/100, Train Loss: 0.7975, Test MSE: 0.5554, Test CI: 0.7935\n",
      "Epoch 36/100, Train Loss: 0.8030, Test MSE: 0.5520, Test CI: 0.7934\n",
      "Epoch 37/100, Train Loss: 0.7816, Test MSE: 0.5642, Test CI: 0.7937\n",
      "Epoch 38/100, Train Loss: 0.7877, Test MSE: 0.5540, Test CI: 0.7946\n",
      "Epoch 39/100, Train Loss: 0.7864, Test MSE: 0.5554, Test CI: 0.7912\n",
      "Epoch 40/100, Train Loss: 0.7753, Test MSE: 0.5556, Test CI: 0.7911\n",
      "Epoch 41/100, Train Loss: 0.7641, Test MSE: 0.5641, Test CI: 0.7923\n",
      "Epoch 42/100, Train Loss: 0.7614, Test MSE: 0.5626, Test CI: 0.7925\n",
      "Epoch 43/100, Train Loss: 0.7526, Test MSE: 0.5569, Test CI: 0.7937\n",
      "Epoch 44/100, Train Loss: 0.7530, Test MSE: 0.5600, Test CI: 0.7893\n",
      "Epoch 45/100, Train Loss: 0.7489, Test MSE: 0.5686, Test CI: 0.7939\n",
      "Epoch 46/100, Train Loss: 0.7371, Test MSE: 0.5554, Test CI: 0.7938\n",
      "Epoch 47/100, Train Loss: 0.7283, Test MSE: 0.5614, Test CI: 0.7918\n",
      "Epoch 48/100, Train Loss: 0.7441, Test MSE: 0.5593, Test CI: 0.7933\n",
      "Epoch 49/100, Train Loss: 0.7322, Test MSE: 0.5586, Test CI: 0.7931\n",
      "Epoch 50/100, Train Loss: 0.7250, Test MSE: 0.5591, Test CI: 0.7923\n",
      "Epoch 51/100, Train Loss: 0.7188, Test MSE: 0.5638, Test CI: 0.7911\n",
      "Epoch 52/100, Train Loss: 0.7158, Test MSE: 0.5559, Test CI: 0.7938\n",
      "Epoch 53/100, Train Loss: 0.7082, Test MSE: 0.5634, Test CI: 0.7925\n",
      "Epoch 54/100, Train Loss: 0.7088, Test MSE: 0.5532, Test CI: 0.7944\n",
      "Epoch 55/100, Train Loss: 0.7034, Test MSE: 0.5535, Test CI: 0.7921\n",
      "Epoch 56/100, Train Loss: 0.7016, Test MSE: 0.5552, Test CI: 0.7924\n",
      "Epoch 57/100, Train Loss: 0.6953, Test MSE: 0.5538, Test CI: 0.7933\n",
      "Epoch 58/100, Train Loss: 0.6870, Test MSE: 0.5623, Test CI: 0.7935\n",
      "Epoch 59/100, Train Loss: 0.6836, Test MSE: 0.5558, Test CI: 0.7920\n",
      "Epoch 60/100, Train Loss: 0.6843, Test MSE: 0.5522, Test CI: 0.7948\n",
      "Epoch 61/100, Train Loss: 0.6854, Test MSE: 0.5529, Test CI: 0.7939\n",
      "Epoch 62/100, Train Loss: 0.6724, Test MSE: 0.5635, Test CI: 0.7921\n",
      "Epoch 63/100, Train Loss: 0.6675, Test MSE: 0.5654, Test CI: 0.7936\n",
      "Epoch 64/100, Train Loss: 0.6591, Test MSE: 0.5550, Test CI: 0.7948\n",
      "Epoch 65/100, Train Loss: 0.6622, Test MSE: 0.5530, Test CI: 0.7935\n",
      "Epoch 66/100, Train Loss: 0.6576, Test MSE: 0.5563, Test CI: 0.7945\n",
      "Epoch 67/100, Train Loss: 0.6539, Test MSE: 0.5409, Test CI: 0.7973\n",
      "Epoch 68/100, Train Loss: 0.6461, Test MSE: 0.5383, Test CI: 0.7966\n",
      "Epoch 69/100, Train Loss: 0.6434, Test MSE: 0.5352, Test CI: 0.7999\n",
      "Epoch 70/100, Train Loss: 0.6325, Test MSE: 0.5313, Test CI: 0.7989\n",
      "Epoch 71/100, Train Loss: 0.6234, Test MSE: 0.5307, Test CI: 0.7979\n",
      "Epoch 72/100, Train Loss: 0.6202, Test MSE: 0.5382, Test CI: 0.7996\n",
      "Epoch 73/100, Train Loss: 0.6195, Test MSE: 0.5399, Test CI: 0.8007\n",
      "Epoch 74/100, Train Loss: 0.6177, Test MSE: 0.5305, Test CI: 0.7994\n",
      "Epoch 75/100, Train Loss: 0.6180, Test MSE: 0.5353, Test CI: 0.7993\n",
      "Epoch 76/100, Train Loss: 0.6066, Test MSE: 0.5331, Test CI: 0.8004\n",
      "Epoch 77/100, Train Loss: 0.6049, Test MSE: 0.5236, Test CI: 0.8003\n",
      "Epoch 78/100, Train Loss: 0.6015, Test MSE: 0.5319, Test CI: 0.7986\n",
      "Epoch 79/100, Train Loss: 0.6013, Test MSE: 0.5293, Test CI: 0.7989\n",
      "Epoch 80/100, Train Loss: 0.5913, Test MSE: 0.5231, Test CI: 0.8006\n",
      "Epoch 81/100, Train Loss: 0.5979, Test MSE: 0.5345, Test CI: 0.7996\n",
      "Epoch 82/100, Train Loss: 0.5923, Test MSE: 0.5249, Test CI: 0.8013\n",
      "Epoch 83/100, Train Loss: 0.5899, Test MSE: 0.5251, Test CI: 0.8019\n",
      "Epoch 84/100, Train Loss: 0.5833, Test MSE: 0.5217, Test CI: 0.7983\n",
      "Epoch 85/100, Train Loss: 0.5850, Test MSE: 0.5229, Test CI: 0.7973\n",
      "Epoch 86/100, Train Loss: 0.5735, Test MSE: 0.5211, Test CI: 0.8011\n",
      "Epoch 87/100, Train Loss: 0.5653, Test MSE: 0.5176, Test CI: 0.8019\n",
      "Epoch 88/100, Train Loss: 0.5678, Test MSE: 0.5237, Test CI: 0.8018\n",
      "Epoch 89/100, Train Loss: 0.5650, Test MSE: 0.5129, Test CI: 0.8024\n",
      "Epoch 90/100, Train Loss: 0.5625, Test MSE: 0.5136, Test CI: 0.8042\n",
      "Epoch 91/100, Train Loss: 0.5656, Test MSE: 0.5209, Test CI: 0.8039\n",
      "Epoch 92/100, Train Loss: 0.5548, Test MSE: 0.5145, Test CI: 0.8046\n",
      "Epoch 93/100, Train Loss: 0.5551, Test MSE: 0.5182, Test CI: 0.8046\n",
      "Epoch 94/100, Train Loss: 0.5583, Test MSE: 0.5132, Test CI: 0.8052\n",
      "Epoch 95/100, Train Loss: 0.5508, Test MSE: 0.5097, Test CI: 0.8060\n",
      "Epoch 96/100, Train Loss: 0.5487, Test MSE: 0.5108, Test CI: 0.8045\n",
      "Epoch 97/100, Train Loss: 0.5402, Test MSE: 0.5119, Test CI: 0.8056\n",
      "Epoch 98/100, Train Loss: 0.5465, Test MSE: 0.5148, Test CI: 0.8036\n",
      "Epoch 99/100, Train Loss: 0.5380, Test MSE: 0.5100, Test CI: 0.8065\n",
      "Epoch 100/100, Train Loss: 0.5341, Test MSE: 0.5127, Test CI: 0.8054\n",
      "Best Test MSE: 0.5097, Best Test CI: 0.8060\n",
      "BidirectionalLSTM - Final MSE: 0.5097, Final CI: 0.8060\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMWithCrossAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(LSTMWithCrossAttention, self).__init__()\n",
    "        self.drug_embedding = nn.Embedding(config['charsmiset_size'] + 1, config['embed_dim'])\n",
    "        self.protein_embedding = nn.Embedding(config['charseqset_size'] + 1, config['embed_dim'])\n",
    "\n",
    "        self.drug_lstm = nn.LSTM(config['embed_dim'], config['lstm_dim'],\n",
    "                                 num_layers=config['lstm_layers'], batch_first=True, bidirectional=True)\n",
    "        self.protein_lstm = nn.LSTM(config['embed_dim'], config['lstm_dim'],\n",
    "                                    num_layers=config['lstm_layers'], batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.drug_attention = nn.Linear(config['lstm_dim'] * 2, config['lstm_dim'] * 2)\n",
    "        self.protein_attention = nn.Linear(config['lstm_dim'] * 2, config['lstm_dim'] * 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(config['lstm_dim'] * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(config['dropout_rate'])\n",
    "\n",
    "    def cross_attention(self, query, key, value):\n",
    "        # query: [batch_size, query_len, dim]\n",
    "        # key: [batch_size, key_len, dim]\n",
    "        # value: [batch_size, key_len, dim]\n",
    "\n",
    "        attention_scores = torch.bmm(query, key.transpose(1, 2))\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        context_vector = torch.bmm(attention_weights, value)\n",
    "        return context_vector\n",
    "\n",
    "    def forward(self, drug, protein):\n",
    "        drug_embedded = self.drug_embedding(drug)\n",
    "        protein_embedded = self.protein_embedding(protein)\n",
    "\n",
    "        drug_output, _ = self.drug_lstm(drug_embedded)\n",
    "        protein_output, _ = self.protein_lstm(protein_embedded)\n",
    "\n",
    "        # Cross-attention\n",
    "        drug_query = self.drug_attention(drug_output)\n",
    "        protein_query = self.protein_attention(protein_output)\n",
    "\n",
    "        drug_context = self.cross_attention(drug_query, protein_output, protein_output)\n",
    "        protein_context = self.cross_attention(protein_query, drug_output, drug_output)\n",
    "\n",
    "        # Global average pooling\n",
    "        drug_repr = torch.mean(drug_context, dim=1)\n",
    "        protein_repr = torch.mean(protein_context, dim=1)\n",
    "\n",
    "        concat = torch.cat([drug_repr, protein_repr], dim=1)\n",
    "\n",
    "        fc1 = F.relu(self.fc1(concat))\n",
    "        fc1 = self.dropout(fc1)\n",
    "        fc2 = F.relu(self.fc2(fc1))\n",
    "        fc2 = self.dropout(fc2)\n",
    "        output = self.fc3(fc2)\n",
    "\n",
    "        return output.squeeze(-1)\n",
    "\n",
    "config = {\n",
    "        'charsmiset_size': 64,\n",
    "        'charseqset_size': 26,\n",
    "        'embed_dim': 128,\n",
    "        'lstm_dim': 64,\n",
    "        'lstm_layers': 2,\n",
    "        'dropout_rate': 0.3,\n",
    "        'learning_rate': 0.0005,\n",
    "        'batch_size': 128,\n",
    "\n",
    "        # Dataset specific\n",
    "        'davis_convert_to_log': True,\n",
    "        'kiba_convert_to_log': False,\n",
    "\n",
    "        # Input dimensions\n",
    "        'max_seq_len': 1000,\n",
    "        'max_smi_len': 100,\n",
    "        'davis_path': '/content/drive/MyDrive/BioInf-Final/data/davis/',\n",
    "        'kiba_path': '/content/drive/MyDrive/BioInf-Final/data/kiba/',\n",
    "        'problem_type': 1\n",
    "\n",
    "}\n",
    "\n",
    "train_dataset, test_dataset = get_datasets('davis', config)\n",
    "\n",
    "\n",
    "mse, ci = run_experiment(LSTMWithCrossAttention, config, train_dataset, test_dataset)\n",
    "print(f\"LSTM with Cross-Attention - Final MSE: {mse:.4f}, Final CI: {ci:.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDCm5hAS3s6_",
    "outputId": "bf10483c-c227-4ea4-8c3f-13e9eee66b07"
   },
   "id": "vDCm5hAS3s6_",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100, Train Loss: 3.7172, Test MSE: 0.7958, Test CI: 0.5358\n",
      "Epoch 2/100, Train Loss: 1.3072, Test MSE: 0.7662, Test CI: 0.6014\n",
      "Epoch 3/100, Train Loss: 1.2361, Test MSE: 0.7402, Test CI: 0.6447\n",
      "Epoch 4/100, Train Loss: 1.1697, Test MSE: 0.7410, Test CI: 0.6977\n",
      "Epoch 5/100, Train Loss: 1.1333, Test MSE: 0.6272, Test CI: 0.7312\n",
      "Epoch 6/100, Train Loss: 1.0788, Test MSE: 0.5924, Test CI: 0.7552\n",
      "Epoch 7/100, Train Loss: 1.0467, Test MSE: 0.6160, Test CI: 0.7737\n",
      "Epoch 8/100, Train Loss: 1.0219, Test MSE: 0.5764, Test CI: 0.7764\n",
      "Epoch 9/100, Train Loss: 1.0198, Test MSE: 0.5644, Test CI: 0.7882\n",
      "Epoch 10/100, Train Loss: 0.9859, Test MSE: 0.5471, Test CI: 0.7868\n",
      "Epoch 11/100, Train Loss: 0.9643, Test MSE: 0.5394, Test CI: 0.7866\n",
      "Epoch 12/100, Train Loss: 0.9513, Test MSE: 0.5298, Test CI: 0.7910\n",
      "Epoch 13/100, Train Loss: 0.9332, Test MSE: 0.5496, Test CI: 0.7964\n",
      "Epoch 14/100, Train Loss: 0.9268, Test MSE: 0.5153, Test CI: 0.8012\n",
      "Epoch 15/100, Train Loss: 0.9037, Test MSE: 0.5282, Test CI: 0.7979\n",
      "Epoch 16/100, Train Loss: 0.9042, Test MSE: 0.5572, Test CI: 0.7988\n",
      "Epoch 17/100, Train Loss: 0.8570, Test MSE: 0.4872, Test CI: 0.7988\n",
      "Epoch 18/100, Train Loss: 0.8554, Test MSE: 0.4857, Test CI: 0.8047\n",
      "Epoch 19/100, Train Loss: 0.8550, Test MSE: 0.4686, Test CI: 0.8076\n",
      "Epoch 20/100, Train Loss: 0.8223, Test MSE: 0.5034, Test CI: 0.8054\n",
      "Epoch 21/100, Train Loss: 0.7996, Test MSE: 0.4597, Test CI: 0.8105\n",
      "Epoch 22/100, Train Loss: 0.7833, Test MSE: 0.4313, Test CI: 0.8167\n",
      "Epoch 23/100, Train Loss: 0.7621, Test MSE: 0.4236, Test CI: 0.8196\n",
      "Epoch 24/100, Train Loss: 0.7549, Test MSE: 0.4250, Test CI: 0.8233\n",
      "Epoch 25/100, Train Loss: 0.7085, Test MSE: 0.4347, Test CI: 0.8347\n",
      "Epoch 26/100, Train Loss: 0.7026, Test MSE: 0.3882, Test CI: 0.8345\n",
      "Epoch 27/100, Train Loss: 0.6897, Test MSE: 0.3819, Test CI: 0.8362\n",
      "Epoch 28/100, Train Loss: 0.6748, Test MSE: 0.3921, Test CI: 0.8360\n",
      "Epoch 29/100, Train Loss: 0.6631, Test MSE: 0.3773, Test CI: 0.8418\n",
      "Epoch 30/100, Train Loss: 0.6615, Test MSE: 0.3796, Test CI: 0.8428\n",
      "Epoch 31/100, Train Loss: 0.6291, Test MSE: 0.3778, Test CI: 0.8417\n",
      "Epoch 32/100, Train Loss: 0.6195, Test MSE: 0.3759, Test CI: 0.8378\n",
      "Epoch 33/100, Train Loss: 0.6052, Test MSE: 0.3767, Test CI: 0.8487\n",
      "Epoch 34/100, Train Loss: 0.5987, Test MSE: 0.3838, Test CI: 0.8383\n",
      "Epoch 35/100, Train Loss: 0.5848, Test MSE: 0.3481, Test CI: 0.8530\n",
      "Epoch 36/100, Train Loss: 0.5655, Test MSE: 0.3539, Test CI: 0.8457\n",
      "Epoch 37/100, Train Loss: 0.5587, Test MSE: 0.3436, Test CI: 0.8536\n",
      "Epoch 38/100, Train Loss: 0.5454, Test MSE: 0.3476, Test CI: 0.8459\n",
      "Epoch 39/100, Train Loss: 0.5150, Test MSE: 0.3437, Test CI: 0.8534\n",
      "Epoch 40/100, Train Loss: 0.5190, Test MSE: 0.3326, Test CI: 0.8559\n",
      "Epoch 41/100, Train Loss: 0.5010, Test MSE: 0.3514, Test CI: 0.8576\n",
      "Epoch 42/100, Train Loss: 0.4885, Test MSE: 0.3661, Test CI: 0.8579\n",
      "Epoch 43/100, Train Loss: 0.4745, Test MSE: 0.3472, Test CI: 0.8587\n",
      "Epoch 44/100, Train Loss: 0.4622, Test MSE: 0.3349, Test CI: 0.8549\n",
      "Epoch 45/100, Train Loss: 0.4577, Test MSE: 0.3417, Test CI: 0.8553\n",
      "Epoch 46/100, Train Loss: 0.4526, Test MSE: 0.3163, Test CI: 0.8601\n",
      "Epoch 47/100, Train Loss: 0.4251, Test MSE: 0.3345, Test CI: 0.8546\n",
      "Epoch 48/100, Train Loss: 0.4202, Test MSE: 0.3136, Test CI: 0.8592\n",
      "Epoch 49/100, Train Loss: 0.4118, Test MSE: 0.3094, Test CI: 0.8637\n",
      "Epoch 50/100, Train Loss: 0.4033, Test MSE: 0.3043, Test CI: 0.8632\n",
      "Epoch 51/100, Train Loss: 0.4030, Test MSE: 0.3067, Test CI: 0.8594\n",
      "Epoch 52/100, Train Loss: 0.3929, Test MSE: 0.3135, Test CI: 0.8633\n",
      "Epoch 53/100, Train Loss: 0.3792, Test MSE: 0.3049, Test CI: 0.8582\n",
      "Epoch 54/100, Train Loss: 0.3656, Test MSE: 0.3059, Test CI: 0.8529\n",
      "Epoch 55/100, Train Loss: 0.3543, Test MSE: 0.3137, Test CI: 0.8644\n",
      "Epoch 56/100, Train Loss: 0.3456, Test MSE: 0.3161, Test CI: 0.8628\n",
      "Epoch 57/100, Train Loss: 0.3454, Test MSE: 0.2992, Test CI: 0.8607\n",
      "Epoch 58/100, Train Loss: 0.3261, Test MSE: 0.3146, Test CI: 0.8548\n",
      "Epoch 59/100, Train Loss: 0.3211, Test MSE: 0.3109, Test CI: 0.8606\n",
      "Epoch 60/100, Train Loss: 0.3081, Test MSE: 0.3060, Test CI: 0.8658\n",
      "Epoch 61/100, Train Loss: 0.3065, Test MSE: 0.3042, Test CI: 0.8664\n",
      "Epoch 62/100, Train Loss: 0.2913, Test MSE: 0.3110, Test CI: 0.8588\n",
      "Epoch 63/100, Train Loss: 0.2865, Test MSE: 0.3060, Test CI: 0.8657\n",
      "Epoch 64/100, Train Loss: 0.2796, Test MSE: 0.3082, Test CI: 0.8602\n",
      "Epoch 65/100, Train Loss: 0.2742, Test MSE: 0.3077, Test CI: 0.8621\n",
      "Epoch 66/100, Train Loss: 0.2655, Test MSE: 0.2994, Test CI: 0.8671\n",
      "Epoch 67/100, Train Loss: 0.2570, Test MSE: 0.3001, Test CI: 0.8622\n",
      "Epoch 68/100, Train Loss: 0.2529, Test MSE: 0.2966, Test CI: 0.8665\n",
      "Epoch 69/100, Train Loss: 0.2500, Test MSE: 0.3073, Test CI: 0.8638\n",
      "Epoch 70/100, Train Loss: 0.2424, Test MSE: 0.3059, Test CI: 0.8624\n",
      "Epoch 71/100, Train Loss: 0.2332, Test MSE: 0.2915, Test CI: 0.8674\n",
      "Epoch 72/100, Train Loss: 0.2322, Test MSE: 0.3006, Test CI: 0.8633\n",
      "Epoch 73/100, Train Loss: 0.2249, Test MSE: 0.3080, Test CI: 0.8613\n",
      "Epoch 74/100, Train Loss: 0.2196, Test MSE: 0.2975, Test CI: 0.8695\n",
      "Epoch 75/100, Train Loss: 0.2156, Test MSE: 0.2950, Test CI: 0.8665\n",
      "Epoch 76/100, Train Loss: 0.2136, Test MSE: 0.3017, Test CI: 0.8625\n",
      "Epoch 77/100, Train Loss: 0.2041, Test MSE: 0.2936, Test CI: 0.8698\n",
      "Epoch 78/100, Train Loss: 0.2016, Test MSE: 0.3021, Test CI: 0.8667\n",
      "Epoch 79/100, Train Loss: 0.1951, Test MSE: 0.2893, Test CI: 0.8704\n",
      "Epoch 80/100, Train Loss: 0.1922, Test MSE: 0.3020, Test CI: 0.8684\n",
      "Epoch 81/100, Train Loss: 0.1887, Test MSE: 0.2982, Test CI: 0.8666\n",
      "Epoch 82/100, Train Loss: 0.1843, Test MSE: 0.3021, Test CI: 0.8700\n",
      "Epoch 83/100, Train Loss: 0.1778, Test MSE: 0.2974, Test CI: 0.8688\n",
      "Epoch 84/100, Train Loss: 0.1723, Test MSE: 0.2976, Test CI: 0.8712\n",
      "Epoch 85/100, Train Loss: 0.1691, Test MSE: 0.2980, Test CI: 0.8691\n",
      "Epoch 86/100, Train Loss: 0.1668, Test MSE: 0.2982, Test CI: 0.8660\n",
      "Epoch 87/100, Train Loss: 0.1651, Test MSE: 0.2911, Test CI: 0.8704\n",
      "Epoch 88/100, Train Loss: 0.1592, Test MSE: 0.2858, Test CI: 0.8714\n",
      "Epoch 89/100, Train Loss: 0.1539, Test MSE: 0.2862, Test CI: 0.8701\n",
      "Epoch 90/100, Train Loss: 0.1527, Test MSE: 0.2887, Test CI: 0.8722\n",
      "Epoch 91/100, Train Loss: 0.1482, Test MSE: 0.2979, Test CI: 0.8675\n",
      "Epoch 92/100, Train Loss: 0.1461, Test MSE: 0.2938, Test CI: 0.8660\n",
      "Epoch 93/100, Train Loss: 0.1424, Test MSE: 0.2879, Test CI: 0.8695\n",
      "Epoch 94/100, Train Loss: 0.1399, Test MSE: 0.2941, Test CI: 0.8729\n",
      "Epoch 95/100, Train Loss: 0.1359, Test MSE: 0.2951, Test CI: 0.8641\n",
      "Epoch 96/100, Train Loss: 0.1356, Test MSE: 0.2969, Test CI: 0.8649\n",
      "Epoch 97/100, Train Loss: 0.1343, Test MSE: 0.2820, Test CI: 0.8725\n",
      "Epoch 98/100, Train Loss: 0.1255, Test MSE: 0.2967, Test CI: 0.8710\n",
      "Epoch 99/100, Train Loss: 0.1247, Test MSE: 0.2834, Test CI: 0.8711\n",
      "Epoch 100/100, Train Loss: 0.1205, Test MSE: 0.2833, Test CI: 0.8686\n",
      "Best Test MSE: 0.2820, Best Test CI: 0.8725\n",
      "LSTM with Cross-Attention - Final MSE: 0.2820, Final CI: 0.8725\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerWithCrossAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TransformerWithCrossAttention, self).__init__()\n",
    "        self.drug_embedding = nn.Embedding(config['charsmiset_size'] + 1, config['d_model'])\n",
    "        self.protein_embedding = nn.Embedding(config['charseqset_size'] + 1, config['d_model'])\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(config['d_model'], config['dropout_rate'])\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config['d_model'],\n",
    "            nhead=config['n_head'],\n",
    "            dim_feedforward=config['d_ff'],\n",
    "            dropout=config['dropout_rate'],\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.drug_encoder = nn.TransformerEncoder(encoder_layer, num_layers=config['n_layers'])\n",
    "        self.protein_encoder = nn.TransformerEncoder(encoder_layer, num_layers=config['n_layers'])\n",
    "\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=config['d_model'],\n",
    "            num_heads=config['n_head'],\n",
    "            dropout=config['dropout_rate'],\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(config['d_model'] * 2, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(config['dropout_rate'])\n",
    "\n",
    "    def forward(self, drug, protein):\n",
    "        drug_embedded = self.drug_embedding(drug)\n",
    "        protein_embedded = self.protein_embedding(protein)\n",
    "\n",
    "        # Apply positional encoding\n",
    "        drug_embedded = self.pos_encoder(drug_embedded.transpose(0, 1)).transpose(0, 1)\n",
    "        protein_embedded = self.pos_encoder(protein_embedded.transpose(0, 1)).transpose(0, 1)\n",
    "\n",
    "        drug_output = self.drug_encoder(drug_embedded)\n",
    "        protein_output = self.protein_encoder(protein_embedded)\n",
    "\n",
    "        # Cross-attention using MultiheadAttention\n",
    "        drug_context, _ = self.cross_attention(drug_output, protein_output, protein_output)\n",
    "        protein_context, _ = self.cross_attention(protein_output, drug_output, drug_output)\n",
    "\n",
    "        # Global average pooling\n",
    "        drug_repr = torch.mean(drug_context, dim=1)\n",
    "        protein_repr = torch.mean(protein_context, dim=1)\n",
    "\n",
    "        concat = torch.cat([drug_repr, protein_repr], dim=1)\n",
    "\n",
    "        fc1 = F.relu(self.fc1(concat))\n",
    "        fc1 = self.dropout(fc1)\n",
    "        fc2 = F.relu(self.fc2(fc1))\n",
    "        fc2 = self.dropout(fc2)\n",
    "        output = self.fc3(fc2)\n",
    "\n",
    "        return output.squeeze(-1)\n",
    "\n",
    "config = {\n",
    "    'charsmiset_size': 64,\n",
    "    'charseqset_size': 26,\n",
    "    'd_model': 128,  # Dimension of the model\n",
    "    'n_head': 8,     # Number of attention heads\n",
    "    'n_layers': 2,   # Number of Transformer layers\n",
    "    'd_ff': 512,     # Dimension of the feedforward network in Transformer\n",
    "    'dropout_rate': 0.1,\n",
    "    'learning_rate': 0.0001,\n",
    "    'batch_size': 64,\n",
    "\n",
    "    # Dataset specific\n",
    "    'davis_convert_to_log': True,\n",
    "    'kiba_convert_to_log': False,\n",
    "\n",
    "    # Input dimensions\n",
    "    'max_seq_len': 1000,\n",
    "    'max_smi_len': 100,\n",
    "    'davis_path': '/content/drive/MyDrive/BioInf-Final/data/davis/',\n",
    "    'kiba_path': '/content/drive/MyDrive/BioInf-Final/data/kiba/',\n",
    "    'problem_type': 1\n",
    "\n",
    "}\n",
    "\n",
    "train_dataset, test_dataset = get_datasets('davis', config)\n",
    "\n",
    "mse, ci = run_experiment(TransformerWithCrossAttention, config, train_dataset, test_dataset)\n",
    "print(f\"Transformer with Cross-Attention - Final MSE: {mse:.4f}, Final CI: {ci:.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iH_6ruWMQgGh",
    "outputId": "d728dc56-77a0-4c0a-e284-015bd1cf57f0"
   },
   "id": "iH_6ruWMQgGh",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100, Train Loss: 3.0500, Test MSE: 0.8052, Test CI: 0.5697\n",
      "Epoch 2/100, Train Loss: 0.9485, Test MSE: 0.7775, Test CI: 0.6392\n",
      "Epoch 3/100, Train Loss: 0.8436, Test MSE: 0.6742, Test CI: 0.7166\n",
      "Epoch 4/100, Train Loss: 0.7923, Test MSE: 0.6392, Test CI: 0.7362\n",
      "Epoch 5/100, Train Loss: 0.7577, Test MSE: 0.6726, Test CI: 0.7464\n",
      "Epoch 6/100, Train Loss: 0.7575, Test MSE: 0.6038, Test CI: 0.7516\n",
      "Epoch 7/100, Train Loss: 0.7314, Test MSE: 0.5865, Test CI: 0.7575\n",
      "Epoch 8/100, Train Loss: 0.6987, Test MSE: 0.5846, Test CI: 0.7689\n",
      "Epoch 9/100, Train Loss: 0.6956, Test MSE: 0.5842, Test CI: 0.7769\n",
      "Epoch 10/100, Train Loss: 0.6743, Test MSE: 0.6608, Test CI: 0.7737\n",
      "Epoch 11/100, Train Loss: 0.6691, Test MSE: 0.5714, Test CI: 0.7823\n",
      "Epoch 12/100, Train Loss: 0.6574, Test MSE: 0.5525, Test CI: 0.7869\n",
      "Epoch 13/100, Train Loss: 0.6528, Test MSE: 0.5301, Test CI: 0.7951\n",
      "Epoch 14/100, Train Loss: 0.6437, Test MSE: 0.5582, Test CI: 0.7846\n",
      "Epoch 15/100, Train Loss: 0.6435, Test MSE: 0.5317, Test CI: 0.7964\n",
      "Epoch 16/100, Train Loss: 0.6268, Test MSE: 0.5212, Test CI: 0.7905\n",
      "Epoch 17/100, Train Loss: 0.6256, Test MSE: 0.5661, Test CI: 0.7856\n",
      "Epoch 18/100, Train Loss: 0.6096, Test MSE: 0.5106, Test CI: 0.7992\n",
      "Epoch 19/100, Train Loss: 0.6175, Test MSE: 0.5214, Test CI: 0.7990\n",
      "Epoch 20/100, Train Loss: 0.6037, Test MSE: 0.5163, Test CI: 0.8007\n",
      "Epoch 21/100, Train Loss: 0.6000, Test MSE: 0.5001, Test CI: 0.8000\n",
      "Epoch 22/100, Train Loss: 0.5956, Test MSE: 0.4948, Test CI: 0.8045\n",
      "Epoch 23/100, Train Loss: 0.5810, Test MSE: 0.5166, Test CI: 0.8040\n",
      "Epoch 24/100, Train Loss: 0.5860, Test MSE: 0.5049, Test CI: 0.8087\n",
      "Epoch 25/100, Train Loss: 0.5742, Test MSE: 0.4872, Test CI: 0.8109\n",
      "Epoch 26/100, Train Loss: 0.5587, Test MSE: 0.4658, Test CI: 0.8078\n",
      "Epoch 27/100, Train Loss: 0.5454, Test MSE: 0.4493, Test CI: 0.8115\n",
      "Epoch 28/100, Train Loss: 0.5407, Test MSE: 0.4455, Test CI: 0.8151\n",
      "Epoch 29/100, Train Loss: 0.5336, Test MSE: 0.4560, Test CI: 0.7971\n",
      "Epoch 30/100, Train Loss: 0.5286, Test MSE: 0.4598, Test CI: 0.8126\n",
      "Epoch 31/100, Train Loss: 0.5271, Test MSE: 0.4666, Test CI: 0.8029\n",
      "Epoch 32/100, Train Loss: 0.5142, Test MSE: 0.4422, Test CI: 0.8155\n",
      "Epoch 33/100, Train Loss: 0.5200, Test MSE: 0.4197, Test CI: 0.8113\n",
      "Epoch 34/100, Train Loss: 0.5130, Test MSE: 0.4236, Test CI: 0.8101\n",
      "Epoch 35/100, Train Loss: 0.5038, Test MSE: 0.4285, Test CI: 0.8181\n",
      "Epoch 36/100, Train Loss: 0.4913, Test MSE: 0.4321, Test CI: 0.8175\n",
      "Epoch 37/100, Train Loss: 0.4988, Test MSE: 0.4420, Test CI: 0.8201\n",
      "Epoch 38/100, Train Loss: 0.4938, Test MSE: 0.4218, Test CI: 0.8096\n",
      "Epoch 39/100, Train Loss: 0.4774, Test MSE: 0.4376, Test CI: 0.8267\n",
      "Epoch 40/100, Train Loss: 0.4796, Test MSE: 0.4392, Test CI: 0.8128\n",
      "Epoch 41/100, Train Loss: 0.4812, Test MSE: 0.4508, Test CI: 0.8282\n",
      "Epoch 42/100, Train Loss: 0.4751, Test MSE: 0.4500, Test CI: 0.8268\n",
      "Epoch 43/100, Train Loss: 0.4742, Test MSE: 0.4289, Test CI: 0.8265\n",
      "Epoch 44/100, Train Loss: 0.4684, Test MSE: 0.4236, Test CI: 0.8226\n",
      "Epoch 45/100, Train Loss: 0.4694, Test MSE: 0.4175, Test CI: 0.8277\n",
      "Epoch 46/100, Train Loss: 0.4600, Test MSE: 0.4001, Test CI: 0.8325\n",
      "Epoch 47/100, Train Loss: 0.4606, Test MSE: 0.3857, Test CI: 0.8320\n",
      "Epoch 48/100, Train Loss: 0.4539, Test MSE: 0.3941, Test CI: 0.8327\n",
      "Epoch 49/100, Train Loss: 0.4546, Test MSE: 0.4164, Test CI: 0.8279\n",
      "Epoch 50/100, Train Loss: 0.4548, Test MSE: 0.3979, Test CI: 0.8310\n",
      "Epoch 51/100, Train Loss: 0.4434, Test MSE: 0.3864, Test CI: 0.8332\n",
      "Epoch 52/100, Train Loss: 0.4450, Test MSE: 0.4050, Test CI: 0.8403\n",
      "Epoch 53/100, Train Loss: 0.4464, Test MSE: 0.4183, Test CI: 0.8321\n",
      "Epoch 54/100, Train Loss: 0.4393, Test MSE: 0.3797, Test CI: 0.8336\n",
      "Epoch 55/100, Train Loss: 0.4326, Test MSE: 0.3935, Test CI: 0.8383\n",
      "Epoch 56/100, Train Loss: 0.4307, Test MSE: 0.4754, Test CI: 0.8283\n",
      "Epoch 57/100, Train Loss: 0.4333, Test MSE: 0.3900, Test CI: 0.8352\n",
      "Epoch 58/100, Train Loss: 0.4267, Test MSE: 0.3776, Test CI: 0.8283\n",
      "Epoch 59/100, Train Loss: 0.4169, Test MSE: 0.4145, Test CI: 0.8444\n",
      "Epoch 60/100, Train Loss: 0.4274, Test MSE: 0.3764, Test CI: 0.8335\n",
      "Epoch 61/100, Train Loss: 0.4149, Test MSE: 0.4080, Test CI: 0.8465\n",
      "Epoch 62/100, Train Loss: 0.4218, Test MSE: 0.3912, Test CI: 0.8436\n",
      "Epoch 63/100, Train Loss: 0.4137, Test MSE: 0.3738, Test CI: 0.8384\n",
      "Epoch 64/100, Train Loss: 0.4144, Test MSE: 0.3715, Test CI: 0.8353\n",
      "Epoch 65/100, Train Loss: 0.4069, Test MSE: 0.3800, Test CI: 0.8436\n",
      "Epoch 66/100, Train Loss: 0.4045, Test MSE: 0.3782, Test CI: 0.8365\n",
      "Epoch 67/100, Train Loss: 0.4017, Test MSE: 0.3694, Test CI: 0.8406\n",
      "Epoch 68/100, Train Loss: 0.4016, Test MSE: 0.4386, Test CI: 0.8504\n",
      "Epoch 69/100, Train Loss: 0.4017, Test MSE: 0.3514, Test CI: 0.8424\n",
      "Epoch 70/100, Train Loss: 0.4019, Test MSE: 0.3738, Test CI: 0.8489\n",
      "Epoch 71/100, Train Loss: 0.3976, Test MSE: 0.3701, Test CI: 0.8431\n",
      "Epoch 72/100, Train Loss: 0.3941, Test MSE: 0.3614, Test CI: 0.8516\n",
      "Epoch 73/100, Train Loss: 0.3883, Test MSE: 0.3803, Test CI: 0.8538\n",
      "Epoch 74/100, Train Loss: 0.3930, Test MSE: 0.3781, Test CI: 0.8468\n",
      "Epoch 75/100, Train Loss: 0.3886, Test MSE: 0.3693, Test CI: 0.8465\n",
      "Epoch 76/100, Train Loss: 0.3824, Test MSE: 0.3866, Test CI: 0.8432\n",
      "Epoch 77/100, Train Loss: 0.3836, Test MSE: 0.3835, Test CI: 0.8541\n",
      "Epoch 78/100, Train Loss: 0.3802, Test MSE: 0.3483, Test CI: 0.8564\n",
      "Epoch 79/100, Train Loss: 0.3780, Test MSE: 0.3554, Test CI: 0.8545\n",
      "Epoch 80/100, Train Loss: 0.3820, Test MSE: 0.3587, Test CI: 0.8500\n",
      "Epoch 81/100, Train Loss: 0.3781, Test MSE: 0.4105, Test CI: 0.8534\n",
      "Epoch 82/100, Train Loss: 0.3663, Test MSE: 0.3699, Test CI: 0.8560\n",
      "Epoch 83/100, Train Loss: 0.3689, Test MSE: 0.3480, Test CI: 0.8449\n",
      "Epoch 84/100, Train Loss: 0.3733, Test MSE: 0.3546, Test CI: 0.8537\n",
      "Epoch 85/100, Train Loss: 0.3646, Test MSE: 0.4064, Test CI: 0.8521\n",
      "Epoch 86/100, Train Loss: 0.3627, Test MSE: 0.3399, Test CI: 0.8573\n",
      "Epoch 87/100, Train Loss: 0.3611, Test MSE: 0.3777, Test CI: 0.8561\n",
      "Epoch 88/100, Train Loss: 0.3622, Test MSE: 0.3523, Test CI: 0.8552\n",
      "Epoch 89/100, Train Loss: 0.3598, Test MSE: 0.3322, Test CI: 0.8526\n",
      "Epoch 90/100, Train Loss: 0.3604, Test MSE: 0.3385, Test CI: 0.8582\n",
      "Epoch 91/100, Train Loss: 0.3499, Test MSE: 0.3353, Test CI: 0.8581\n",
      "Epoch 92/100, Train Loss: 0.3522, Test MSE: 0.3518, Test CI: 0.8508\n",
      "Epoch 93/100, Train Loss: 0.3548, Test MSE: 0.3300, Test CI: 0.8515\n",
      "Epoch 94/100, Train Loss: 0.3488, Test MSE: 0.3897, Test CI: 0.8535\n",
      "Epoch 95/100, Train Loss: 0.3429, Test MSE: 0.3783, Test CI: 0.8623\n",
      "Epoch 96/100, Train Loss: 0.3380, Test MSE: 0.3788, Test CI: 0.8575\n",
      "Epoch 97/100, Train Loss: 0.3467, Test MSE: 0.3360, Test CI: 0.8548\n",
      "Epoch 98/100, Train Loss: 0.3332, Test MSE: 0.3285, Test CI: 0.8597\n",
      "Epoch 99/100, Train Loss: 0.3307, Test MSE: 0.3508, Test CI: 0.8567\n",
      "Epoch 100/100, Train Loss: 0.3319, Test MSE: 0.3336, Test CI: 0.8495\n",
      "Best Test MSE: 0.3285, Best Test CI: 0.8597\n",
      "Transformer with Cross-Attention - Final MSE: 0.3285, Final CI: 0.8597\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    'charsmiset_size': 64,\n",
    "    'charseqset_size': 26,\n",
    "    'd_model': 128,  # Dimension of the model\n",
    "    'n_head': 8,     # Number of attention heads\n",
    "    'n_layers': 2,   # Number of Transformer layers\n",
    "    'd_ff': 512,     # Dimension of the feedforward network in Transformer\n",
    "    'dropout_rate': 0.3,\n",
    "    'learning_rate': 0.0005,\n",
    "    'batch_size': 128,\n",
    "\n",
    "    # Dataset specific\n",
    "    'davis_convert_to_log': True,\n",
    "    'kiba_convert_to_log': False,\n",
    "\n",
    "    # Input dimensions\n",
    "    'max_seq_len': 1000,\n",
    "    'max_smi_len': 100,\n",
    "    'davis_path': '/content/drive/MyDrive/BioInf-Final/data/davis/',\n",
    "    'kiba_path': '/content/drive/MyDrive/BioInf-Final/data/kiba/',\n",
    "    'problem_type': 1\n",
    "\n",
    "}\n",
    "\n",
    "train_dataset, test_dataset = get_datasets('davis', config)\n",
    "\n",
    "\n",
    "mse, ci = run_experiment(TransformerWithCrossAttention, config, train_dataset, test_dataset)\n",
    "print(f\"Transformer with Cross-Attention - Final MSE: {mse:.4f}, Final CI: {ci:.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64ve05i3i5Ch",
    "outputId": "72ca5b38-31d9-47a0-ae8b-ae94609327ed"
   },
   "id": "64ve05i3i5Ch",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epochs:   1%|          | 1/100 [00:46<1:16:06, 46.13s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100, Train Loss: 2.6403, Test MSE: 0.9272, Test CI: 0.5660\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:   2%|▏         | 2/100 [01:32<1:15:16, 46.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2/100, Train Loss: 1.3347, Test MSE: 1.2470, Test CI: 0.5739\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:   3%|▎         | 3/100 [02:18<1:14:28, 46.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3/100, Train Loss: 1.2784, Test MSE: 0.9216, Test CI: 0.6365\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:   4%|▍         | 4/100 [03:04<1:13:42, 46.06s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4/100, Train Loss: 1.2307, Test MSE: 0.8570, Test CI: 0.6770\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:   5%|▌         | 5/100 [03:50<1:12:55, 46.05s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5/100, Train Loss: 1.1630, Test MSE: 1.2556, Test CI: 0.7226\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:   6%|▌         | 6/100 [04:36<1:12:12, 46.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 6/100, Train Loss: 1.1633, Test MSE: 0.8630, Test CI: 0.7303\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:   7%|▋         | 7/100 [05:22<1:11:24, 46.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 7/100, Train Loss: 1.1367, Test MSE: 0.8925, Test CI: 0.7333\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:   8%|▊         | 8/100 [06:08<1:10:36, 46.05s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 8/100, Train Loss: 1.0784, Test MSE: 1.3011, Test CI: 0.7299\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:   9%|▉         | 9/100 [06:54<1:09:50, 46.05s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 9/100, Train Loss: 1.0864, Test MSE: 0.9285, Test CI: 0.7488\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  10%|█         | 10/100 [07:40<1:09:06, 46.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 10/100, Train Loss: 1.0578, Test MSE: 0.7468, Test CI: 0.7499\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  11%|█         | 11/100 [08:26<1:08:19, 46.06s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 11/100, Train Loss: 1.0388, Test MSE: 1.3652, Test CI: 0.7475\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  12%|█▏        | 12/100 [09:12<1:07:34, 46.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 12/100, Train Loss: 1.0549, Test MSE: 0.6287, Test CI: 0.7518\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  13%|█▎        | 13/100 [09:58<1:06:49, 46.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 13/100, Train Loss: 1.0439, Test MSE: 0.7098, Test CI: 0.7599\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  14%|█▍        | 14/100 [10:44<1:05:57, 46.02s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 14/100, Train Loss: 1.0040, Test MSE: 0.7267, Test CI: 0.7602\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  15%|█▌        | 15/100 [11:31<1:05:18, 46.10s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 15/100, Train Loss: 1.0048, Test MSE: 0.8344, Test CI: 0.7670\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  16%|█▌        | 16/100 [12:17<1:04:33, 46.12s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 16/100, Train Loss: 0.9781, Test MSE: 0.9918, Test CI: 0.7634\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  17%|█▋        | 17/100 [13:03<1:03:48, 46.12s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 17/100, Train Loss: 0.9617, Test MSE: 0.9416, Test CI: 0.7723\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  18%|█▊        | 18/100 [13:49<1:03:02, 46.12s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 18/100, Train Loss: 0.9460, Test MSE: 0.7071, Test CI: 0.7688\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  19%|█▉        | 19/100 [14:35<1:02:14, 46.10s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 19/100, Train Loss: 0.9349, Test MSE: 1.0196, Test CI: 0.7732\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  20%|██        | 20/100 [15:21<1:01:27, 46.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 20/100, Train Loss: 0.9407, Test MSE: 0.6775, Test CI: 0.7775\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  21%|██        | 21/100 [16:07<1:00:40, 46.08s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 21/100, Train Loss: 0.9310, Test MSE: 1.1269, Test CI: 0.7798\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  22%|██▏       | 22/100 [16:53<59:55, 46.10s/it]  "
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 22/100, Train Loss: 0.9077, Test MSE: 0.7816, Test CI: 0.7821\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  23%|██▎       | 23/100 [17:40<59:11, 46.13s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 23/100, Train Loss: 0.9037, Test MSE: 1.1175, Test CI: 0.7806\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  24%|██▍       | 24/100 [18:26<58:23, 46.10s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 24/100, Train Loss: 0.8899, Test MSE: 0.6526, Test CI: 0.7855\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  25%|██▌       | 25/100 [19:12<57:38, 46.11s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 25/100, Train Loss: 0.8690, Test MSE: 0.6236, Test CI: 0.7855\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  26%|██▌       | 26/100 [19:58<56:52, 46.11s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 26/100, Train Loss: 0.8593, Test MSE: 0.7266, Test CI: 0.7873\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  27%|██▋       | 27/100 [20:44<56:04, 46.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 27/100, Train Loss: 0.8520, Test MSE: 0.6332, Test CI: 0.7883\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  28%|██▊       | 28/100 [21:30<55:17, 46.08s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 28/100, Train Loss: 0.8504, Test MSE: 0.8797, Test CI: 0.7885\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  29%|██▉       | 29/100 [22:16<54:32, 46.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 29/100, Train Loss: 0.8268, Test MSE: 0.6878, Test CI: 0.7866\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  30%|███       | 30/100 [23:02<53:46, 46.10s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 30/100, Train Loss: 0.8299, Test MSE: 0.6275, Test CI: 0.7885\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  31%|███       | 31/100 [23:48<52:59, 46.08s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 31/100, Train Loss: 0.8032, Test MSE: 0.9292, Test CI: 0.7939\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  32%|███▏      | 32/100 [24:34<52:12, 46.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 32/100, Train Loss: 0.7978, Test MSE: 0.6613, Test CI: 0.7860\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  33%|███▎      | 33/100 [25:20<51:26, 46.06s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 33/100, Train Loss: 0.7833, Test MSE: 0.5536, Test CI: 0.7907\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  34%|███▍      | 34/100 [26:06<50:40, 46.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 34/100, Train Loss: 0.7847, Test MSE: 0.6040, Test CI: 0.7865\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  35%|███▌      | 35/100 [26:52<49:53, 46.06s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 35/100, Train Loss: 0.7691, Test MSE: 0.5935, Test CI: 0.7954\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  36%|███▌      | 36/100 [27:38<49:08, 46.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 36/100, Train Loss: 0.7484, Test MSE: 0.7056, Test CI: 0.7984\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  37%|███▋      | 37/100 [28:25<48:21, 46.06s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 37/100, Train Loss: 0.7324, Test MSE: 0.5971, Test CI: 0.7901\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  38%|███▊      | 38/100 [29:11<47:36, 46.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 38/100, Train Loss: 0.7135, Test MSE: 0.5147, Test CI: 0.7897\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  39%|███▉      | 39/100 [29:57<46:50, 46.08s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 39/100, Train Loss: 0.6905, Test MSE: 0.5145, Test CI: 0.7868\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  40%|████      | 40/100 [30:43<46:04, 46.08s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 40/100, Train Loss: 0.6979, Test MSE: 0.8321, Test CI: 0.7799\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  41%|████      | 41/100 [31:29<45:21, 46.12s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 41/100, Train Loss: 0.6828, Test MSE: 0.5399, Test CI: 0.7963\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  42%|████▏     | 42/100 [32:15<44:34, 46.11s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 42/100, Train Loss: 0.6707, Test MSE: 0.4668, Test CI: 0.8035\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  43%|████▎     | 43/100 [33:01<43:47, 46.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 43/100, Train Loss: 0.6531, Test MSE: 0.6033, Test CI: 0.7962\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  44%|████▍     | 44/100 [33:47<43:00, 46.08s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 44/100, Train Loss: 0.6445, Test MSE: 0.4937, Test CI: 0.8017\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  45%|████▌     | 45/100 [34:33<42:12, 46.05s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 45/100, Train Loss: 0.6489, Test MSE: 0.5564, Test CI: 0.7992\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  46%|████▌     | 46/100 [35:19<41:27, 46.06s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 46/100, Train Loss: 0.6379, Test MSE: 0.5499, Test CI: 0.8074\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  47%|████▋     | 47/100 [36:05<40:40, 46.05s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 47/100, Train Loss: 0.6242, Test MSE: 0.4988, Test CI: 0.7962\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  48%|████▊     | 48/100 [36:51<39:55, 46.06s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 48/100, Train Loss: 0.6174, Test MSE: 0.4630, Test CI: 0.8063\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  49%|████▉     | 49/100 [37:37<39:09, 46.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 49/100, Train Loss: 0.6160, Test MSE: 0.5734, Test CI: 0.7935\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  50%|█████     | 50/100 [38:24<38:24, 46.08s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 50/100, Train Loss: 0.5975, Test MSE: 0.4788, Test CI: 0.8057\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  51%|█████     | 51/100 [39:10<37:38, 46.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 51/100, Train Loss: 0.6038, Test MSE: 0.4670, Test CI: 0.8074\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  52%|█████▏    | 52/100 [39:56<36:53, 46.12s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 52/100, Train Loss: 0.5866, Test MSE: 0.4762, Test CI: 0.8012\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  53%|█████▎    | 53/100 [40:42<36:07, 46.11s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 53/100, Train Loss: 0.5869, Test MSE: 0.4884, Test CI: 0.8075\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  54%|█████▍    | 54/100 [41:28<35:20, 46.10s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 54/100, Train Loss: 0.5702, Test MSE: 0.4484, Test CI: 0.7971\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  55%|█████▌    | 55/100 [42:14<34:33, 46.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 55/100, Train Loss: 0.5597, Test MSE: 0.4580, Test CI: 0.8029\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  56%|█████▌    | 56/100 [43:00<33:46, 46.06s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 56/100, Train Loss: 0.5620, Test MSE: 0.4572, Test CI: 0.8048\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  57%|█████▋    | 57/100 [43:46<33:00, 46.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 57/100, Train Loss: 0.5595, Test MSE: 0.4997, Test CI: 0.8033\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  58%|█████▊    | 58/100 [44:32<32:15, 46.08s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 58/100, Train Loss: 0.5479, Test MSE: 0.4233, Test CI: 0.8157\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  59%|█████▉    | 59/100 [45:18<31:28, 46.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 59/100, Train Loss: 0.5453, Test MSE: 0.4409, Test CI: 0.8156\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  60%|██████    | 60/100 [46:04<30:43, 46.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 60/100, Train Loss: 0.5314, Test MSE: 0.5239, Test CI: 0.8043\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  61%|██████    | 61/100 [46:50<29:56, 46.07s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 61/100, Train Loss: 0.5315, Test MSE: 0.4427, Test CI: 0.8188\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  62%|██████▏   | 62/100 [47:36<29:08, 46.02s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 62/100, Train Loss: 0.5251, Test MSE: 0.4404, Test CI: 0.8155\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  63%|██████▎   | 63/100 [48:22<28:22, 46.02s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 63/100, Train Loss: 0.5190, Test MSE: 0.4519, Test CI: 0.8110\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  64%|██████▍   | 64/100 [49:09<27:39, 46.10s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 64/100, Train Loss: 0.5148, Test MSE: 0.5095, Test CI: 0.8048\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  65%|██████▌   | 65/100 [49:55<26:53, 46.10s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 65/100, Train Loss: 0.5138, Test MSE: 0.4597, Test CI: 0.8206\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  66%|██████▌   | 66/100 [50:41<26:07, 46.11s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 66/100, Train Loss: 0.5036, Test MSE: 0.4520, Test CI: 0.8129\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  67%|██████▋   | 67/100 [51:27<25:21, 46.11s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 67/100, Train Loss: 0.5055, Test MSE: 0.4875, Test CI: 0.8085\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  68%|██████▊   | 68/100 [52:13<24:35, 46.11s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 68/100, Train Loss: 0.4974, Test MSE: 0.4641, Test CI: 0.8137\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  69%|██████▉   | 69/100 [52:59<23:49, 46.10s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 69/100, Train Loss: 0.4884, Test MSE: 0.4703, Test CI: 0.8126\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  70%|███████   | 70/100 [53:45<23:01, 46.04s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 70/100, Train Loss: 0.4893, Test MSE: 0.4505, Test CI: 0.8121\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  71%|███████   | 71/100 [54:31<22:17, 46.13s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 71/100, Train Loss: 0.4953, Test MSE: 0.4724, Test CI: 0.8130\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  72%|███████▏  | 72/100 [55:18<21:31, 46.13s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 72/100, Train Loss: 0.4805, Test MSE: 0.4725, Test CI: 0.7993\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  73%|███████▎  | 73/100 [56:04<20:45, 46.14s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 73/100, Train Loss: 0.4821, Test MSE: 0.4619, Test CI: 0.8182\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  74%|███████▍  | 74/100 [56:50<19:59, 46.14s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 74/100, Train Loss: 0.4779, Test MSE: 0.4890, Test CI: 0.8182\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  75%|███████▌  | 75/100 [57:36<19:13, 46.12s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 75/100, Train Loss: 0.4688, Test MSE: 0.4672, Test CI: 0.8182\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  76%|███████▌  | 76/100 [58:22<18:26, 46.11s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 76/100, Train Loss: 0.4671, Test MSE: 0.4428, Test CI: 0.8155\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  77%|███████▋  | 77/100 [59:08<17:40, 46.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 77/100, Train Loss: 0.4662, Test MSE: 0.4478, Test CI: 0.8246\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  78%|███████▊  | 78/100 [59:54<16:54, 46.10s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 78/100, Train Loss: 0.4590, Test MSE: 0.4345, Test CI: 0.8186\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  79%|███████▉  | 79/100 [1:00:40<16:06, 46.05s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 79/100, Train Loss: 0.4634, Test MSE: 0.4799, Test CI: 0.8147\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  80%|████████  | 80/100 [1:01:26<15:22, 46.12s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 80/100, Train Loss: 0.4582, Test MSE: 0.4615, Test CI: 0.8166\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  81%|████████  | 81/100 [1:02:13<14:36, 46.13s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 81/100, Train Loss: 0.4591, Test MSE: 0.4812, Test CI: 0.8102\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  82%|████████▏ | 82/100 [1:02:59<13:50, 46.13s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 82/100, Train Loss: 0.4441, Test MSE: 0.5079, Test CI: 0.8107\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  83%|████████▎ | 83/100 [1:03:45<13:04, 46.13s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 83/100, Train Loss: 0.4469, Test MSE: 0.4664, Test CI: 0.8077\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  84%|████████▍ | 84/100 [1:04:31<12:17, 46.11s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 84/100, Train Loss: 0.4466, Test MSE: 0.4550, Test CI: 0.8108\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  85%|████████▌ | 85/100 [1:05:17<11:31, 46.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 85/100, Train Loss: 0.4375, Test MSE: 0.4757, Test CI: 0.8123\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  86%|████████▌ | 86/100 [1:06:03<10:45, 46.10s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 86/100, Train Loss: 0.4298, Test MSE: 0.4487, Test CI: 0.8187\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  87%|████████▋ | 87/100 [1:06:49<09:59, 46.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 87/100, Train Loss: 0.4368, Test MSE: 0.4324, Test CI: 0.8211\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  88%|████████▊ | 88/100 [1:07:35<09:12, 46.08s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 88/100, Train Loss: 0.4302, Test MSE: 0.4561, Test CI: 0.8161\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  89%|████████▉ | 89/100 [1:08:21<08:26, 46.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 89/100, Train Loss: 0.4249, Test MSE: 0.4413, Test CI: 0.8266\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  90%|█████████ | 90/100 [1:09:07<07:40, 46.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 90/100, Train Loss: 0.4219, Test MSE: 0.4484, Test CI: 0.8240\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  91%|█████████ | 91/100 [1:09:53<06:54, 46.04s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 91/100, Train Loss: 0.4202, Test MSE: 0.5443, Test CI: 0.8159\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  92%|█████████▏| 92/100 [1:10:40<06:08, 46.11s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 92/100, Train Loss: 0.4195, Test MSE: 0.4279, Test CI: 0.8277\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  93%|█████████▎| 93/100 [1:11:26<05:22, 46.13s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 93/100, Train Loss: 0.4158, Test MSE: 0.4709, Test CI: 0.8113\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  94%|█████████▍| 94/100 [1:12:12<04:36, 46.13s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 94/100, Train Loss: 0.4077, Test MSE: 0.4693, Test CI: 0.8214\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  95%|█████████▌| 95/100 [1:12:58<03:50, 46.14s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 95/100, Train Loss: 0.4039, Test MSE: 0.4884, Test CI: 0.8232\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  96%|█████████▌| 96/100 [1:13:44<03:04, 46.10s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 96/100, Train Loss: 0.4032, Test MSE: 0.4651, Test CI: 0.8265\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  97%|█████████▋| 97/100 [1:14:30<02:18, 46.10s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 97/100, Train Loss: 0.4069, Test MSE: 0.4912, Test CI: 0.8141\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  98%|█████████▊| 98/100 [1:15:16<01:32, 46.11s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 98/100, Train Loss: 0.4030, Test MSE: 0.4630, Test CI: 0.8289\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:  99%|█████████▉| 99/100 [1:16:02<00:46, 46.05s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 99/100, Train Loss: 0.3956, Test MSE: 0.5091, Test CI: 0.8112\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epochs: 100%|██████████| 100/100 [1:16:49<00:00, 46.09s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 100/100, Train Loss: 0.3867, Test MSE: 0.4676, Test CI: 0.8262\n",
      "Best Test MSE: 0.4233, Best Test CI: 0.8157\n",
      "Transformer with Cross-Attention - Final MSE: 0.4233, Final CI: 0.8157\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Tested Different Architectures, Bi-Directional LSTM With Cross Attention Was Best"
   ],
   "metadata": {
    "id": "T272bHW85a6b"
   },
   "id": "T272bHW85a6b",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "config = {\n",
    "        # Model architecture\n",
    "        'model_type': 'lstm',\n",
    "        'lstm_layers': 1,\n",
    "        'lstm_dim': 32,\n",
    "        'dense_layers': 1,\n",
    "        'dense_dim': 32,\n",
    "        'dropout_rate': 0.2,\n",
    "        'embed_dim': 64,\n",
    "\n",
    "        # Training parameters\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 64,\n",
    "        'num_epochs': 100,\n",
    "\n",
    "        # Input dimensions\n",
    "        'max_seq_len': 1000,\n",
    "        'max_smi_len': 100,\n",
    "\n",
    "        # Data handling\n",
    "        'davis_path': '/content/drive/MyDrive/BioInf-Final/data/davis/',\n",
    "        'kiba_path': '/content/drive/MyDrive/BioInf-Final/data/kiba/',\n",
    "        'problem_type': 1,\n",
    "        'binary_th': 0.0,\n",
    "\n",
    "        # Output and logging\n",
    "        'checkpoint_path': '',\n",
    "        'log_dir': 'logs',\n",
    "\n",
    "        # Dataset specific\n",
    "        'davis_convert_to_log': True,\n",
    "        'kiba_convert_to_log': False,\n",
    "\n",
    "        # Alphabet sizes\n",
    "        'charsmiset_size': 64,  # Size of SMILES alphabet\n",
    "        'charseqset_size': 26,  # Size of protein sequence alphabet\n",
    "    }\n",
    "\n",
    "dataset = DataSet(config['kiba_path'], config['problem_type'], config['max_seq_len'],\n",
    "                      config['max_smi_len'], 'kiba', config['kiba_convert_to_log'])\n",
    "\n",
    "XD, XT, Y, label_row_inds, label_col_inds, test_fold, train_folds = dataset.get_data()\n",
    "\n",
    "train_fold = unite_lists(train_folds)\n",
    "\n",
    "train_drugs, train_targets, train_affinity = prepare_interaction_pairs(XD, XT, Y, label_row_inds[train_fold], label_col_inds[train_fold])\n",
    "test_drugs, test_targets, test_affinity = prepare_interaction_pairs(XD, XT, Y, label_row_inds[test_fold], label_col_inds[test_fold])\n",
    "\n",
    "train_dataset_kiba = DTIDataset(train_drugs, train_targets, train_affinity)\n",
    "test_dataset_kiba = DTIDataset(test_drugs, test_targets, test_affinity)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sOlmxMur54sR",
    "outputId": "070055e1-63ba-41d4-9b8c-1f24c14ad0be"
   },
   "id": "sOlmxMur54sR",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading kiba dataset from /content/drive/MyDrive/BioInf-Final/data/kiba/\n",
      "Parsing kiba dataset\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "config = {\n",
    "        'charsmiset_size': 64,\n",
    "        'charseqset_size': 26,\n",
    "        'embed_dim': 128,\n",
    "        'lstm_dim': 64,\n",
    "        'lstm_layers': 2,\n",
    "        'dropout_rate': 0.3,\n",
    "        'learning_rate': 0.0005,\n",
    "        'batch_size': 128,\n",
    "\n",
    "        # Dataset specific\n",
    "        'davis_convert_to_log': True,\n",
    "        'kiba_convert_to_log': False,\n",
    "\n",
    "        # Input dimensions\n",
    "        'max_seq_len': 1000,\n",
    "        'max_smi_len': 100,\n",
    "        'davis_path': '/content/drive/MyDrive/BioInf-Final/data/davis/',\n",
    "        'kiba_path': '/content/drive/MyDrive/BioInf-Final/data/kiba/',\n",
    "        'problem_type': 1\n",
    "\n",
    "}\n",
    "\n",
    "train_dataset, test_dataset = get_datasets('kiba', config)\n",
    "\n",
    "\n",
    "mse, ci = run_experiment(LSTMWithCrossAttention, config, train_dataset, test_dataset)\n",
    "print(f\"LSTM with Cross-Attention - Final MSE: {mse:.4f}, Final CI: {ci:.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WX5jKRu_6MZ2",
    "outputId": "19ac011c-4212-4dd9-d057-17feb4e6a82e"
   },
   "id": "WX5jKRu_6MZ2",
   "execution_count": null,
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   1%|          | 1/100 [05:15<8:41:12, 315.88s/it]"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 6.3522, Test MSE: 0.6836, Test CI: 0.5935\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEpochs:   2%|▏         | 2/100 [10:31<8:35:29, 315.61s/it]"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Train Loss: 2.8758, Test MSE: 0.6793, Test CI: 0.6408\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _xla_gc_callback at 0x7854a480d510>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 98, in _xla_gc_callback\n",
      "    def _xla_gc_callback(*args):\n",
      "KeyboardInterrupt: \n",
      "Epochs:   3%|▎         | 3/100 [15:45<8:28:49, 314.74s/it]"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Train Loss: 2.7781, Test MSE: 0.6370, Test CI: 0.6548\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rEpochs:   4%|▍         | 4/100 [20:57<8:22:17, 313.93s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4/100, Train Loss: 2.6347, Test MSE: 0.7229, Test CI: 0.6784\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Epoch 1/100, Train Loss: 2.9745, Test MSE: 0.7973, Test CI: 0.5264\n",
    "# Epoch 2/100, Train Loss: 1.3261, Test MSE: 0.7863, Test CI: 0.5359"
   ],
   "metadata": {
    "id": "K1xbnOYktWHf"
   },
   "id": "K1xbnOYktWHf",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
